01_HuggingFace/README.md

# Evidencias Hugging Face

# ğŸš€ Evidencias â€” Hugging Face (Parte 2.1)

Este apartado contiene todas las evidencias solicitadas en la secciÃ³n **2.1 del Laboratorio 8**: exploraciÃ³n de modelos, ejecuciÃ³n en notebook, desarrollo de un Space y anÃ¡lisis tÃ©cnico.

---

## ğŸ“Œ 1. Modelos explorados

Se exploraron 3 modelos representativos de las categorÃ­as requeridas:

### ğŸ”¹ **NLP â€“ distilbert-base-uncased-finetuned-sst-2-english**
Modelo de clasificaciÃ³n de sentimiento basado en BERT, reducido y optimizado.

### ğŸ”¹ **VisiÃ³n â€“ google/vit-base-patch16-224**
Modelo de clasificaciÃ³n de imÃ¡genes basado en Vision Transformer.

### ğŸ”¹ **Audio â€“ facebook/wav2vec2-base-960h**
Modelo para reconocimiento de voz entrenado sobre 960 horas de audio.

---

## ğŸ“Œ 2. Notebook ejecutado (Pipeline NLP)
Se ejecutÃ³ un modelo NLP utilizando el pipeline de **sentiment-analysis** de Hugging Face.

ğŸ“ **Evidencias del notebook:**  
- `Captura de pantalla 2025-11-22 021210.png`  
- `notebook_ejecucion_2.png`  


El cÃ³digo usado fue:

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
result = classifier("I love studying artificial intelligence!")
result

---

## ğŸ“Œ 3. Hugging Face Space publicado

Se creÃ³ un **chatbot funcional** utilizando Gradio y el modelo GPT-2.

ğŸ”— **Enlace al Space:**  
https://huggingface.co/spaces/yysanchez/chatbot_yulieth 

### ğŸ“‚ Archivos incluidos

- `app.py` â†’ CÃ³digo completo del chatbot  
- `requirements.txt` â†’ LibrerÃ­as necesarias  

---

## ğŸ“Œ 4. AnÃ¡lisis TÃ©cnico del Chatbot (GPT-2)

El chatbot fue creado usando **GPT-2**, un modelo basado en la arquitectura Transformer (Decoder Only).  
Este modelo genera texto prediciendo el siguiente token mÃ¡s probable segÃºn el contexto previo.  
Se integrÃ³ mediante el pipeline de `text-generation` de Hugging Face.

### âœ” **Fortalezas**
- RÃ¡pido y ligero para la inferencia.  
- No requiere GPU para ejecutarse en Hugging Face Spaces.  
- FÃ¡cil de integrar con **Gradio**, ideal para prototipos.  
- Adecuado para fines educativos y demostrativos.  

### âœ” **Limitaciones**
- GPT-2 no estÃ¡ alineado a instrucciones modernas.  
- Puede generar respuestas incoherentes si la conversaciÃ³n es larga.  
- No posee memoria extendida ni capacidades de razonamiento.  

### âœ” **Rendimiento**
- Latencia aproximada: **1â€“2 segundos** por respuesta.  
- Funcionamiento estable en CPU dentro del Space.  
- Bajo consumo de memoria comparado con modelos mÃ¡s grandes.  

---



